---
title: "Genotyping-by-Sequencing data SNP and dosage calling - A practical guide for polyploid species"
author: "Cristiane Taniguti"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: pygments
    keep_md: yes
    toc: yes
    toc_depth: '3'
    toc_float:
      collapsed: no
  md_document:
    variant: markdown_github
  pdf_document:
    highlight: pygments
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
linestretch: 1.2
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "#>",
                      fig.width = 16,
                      fig.height = 16,
                      fig.align = "center",
                      dev = "png",
                      cache = TRUE)
```

## SNP calling

Here we run the SNP and dosage calling using [BWA](http://bio-bwa.sourceforge.net/bwa.shtml) and [GATK](https://gatk.broadinstitute.org/hc/en-us) just for a subset of a tetraploid rose GBS data set. This subset contains only four individuals and sequences that aligned to the half of *Rosa chinensis* chromosome 4. Check the RAM and CPU that was used to run the same pipeline for the full data set below:

![Computational resources required to run this pipeline in the rose complete data (mean depth ~83 and population size of 114 progeny)](log10_efficiency_rose.png)

# What you will need to run this tutorial

* [Download the example FASTQ files and the reference genome](https://drive.google.com/drive/folders/1Wc6LgL_hqi-gx7JVl1IlEOxvJ93CuTgP?usp=sharing)
* Linux system
* At least 4G RAM
* At least 7G HD
* Docker or singularity
* Internet connection

**warning**: Before start, it is important to highlight that all software here presented have several parameters that can be adjusted for each given scenario. Here, we will use the default settings, but for the best usage of each one of them, it is important to explore their specificity. So... use it carefully with other data sets!

# Getting all required software

To save time during the tutorial, pull now all the required images:

* Docker

```{bash, eval=FALSE}
docker pull biocontainers/fastqc:v0.11.9_cv7
docker pull ewels/multiqc:latest
docker pull kfdrc/bwa-picard:latest-dev
docker pull cristaniguti/r-samtools:latest
docker pull kfdrc/cutadapt:latest
docker pull taniguti/gatk-picard:latest
docker pull us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z
```

* Singularity

```{bash, eval=FALSE}
mkdir .singularity
cd .singularity

export SINGULARITY_CACHEDIR=/home/.singularity

singularity pull biocontainers_fastqc_v0.11.9_cv7.sif docker://biocontainers/fastqc:v0.11.9_cv7
singularity pull ewels_multiqc.sif docker://ewels/multiqc:latest
singularity pull kfdrc_bwa-picard_latest-dev.sif docker://kfdrc/bwa-picard:latest-dev
singularity pull cristaniguti_r-samtools_latest.sif docker://cristaniguti/r-samtools:latest
singularity pull kfdrc_cutadapt_latest.sif docker://kfdrc/cutadapt:latest
singularity pull taniguti_gatk-picard_latest.sif docker://taniguti/gatk-picard:latest
singularity pull us.gcr.io_broad-gotc-prod_genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z.sif docker://us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z
```

Run this command and go take a coffee, it can take some time. If you are using Docker, don't forget to remove all images and containers after you finish your analysis to avoid occupy space in the computer.

# Quality Control

A classic way to have a overview of our FASTQ sequences is the FASTQC software:

```{bash, eval=FALSE}
for i in *sub.fastq; do # Will run in all files in the directory finishing with sub.fastq
    echo $i
    docker run -v $(pwd):/opt biocontainers/fastqc:v0.11.9_cv7 fastqc /opt/$i
done
```

or

```{bash, eval=FALSE}
for i in *.fastq; do # Will run in all files in the directory finishing with sub.fastq
    echo $i
    singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/biocontainers_fastqc_v0.11.9_cv7.sif fastqc /opt/$i
done
```

It returns to us an HTML file with several precious pieces of information. 
Instead of checking separated HTML by sample, we can join them using the very fancy MULTIQC tool. 


```{bash, eval=FALSE}
singularity run $SCRATCH/.singularity/ewels_multiqc.sif . --title "Tutorial samples"
```

Results [here](https://cristianetaniguti.github.io/gvenck2022/Tutorial-samples_multiqc_report.html).

This profile is typical from GBS sequences, other types of libraries would return other profiles and could have different problems. 

Now that we already checked how our sequences are, we can decide which filters to use to clean them. Here, the samples are already demultiplexed, but if it is not, process_radtags from STACKs perform the demultiplexing and some other quality filters, as searching for the enzyme cut site and according to sequence Phred scale. Check its manual [here](https://catchenlab.life.illinois.edu/stacks/comp/process_radtags.php). Our samples presented presence of adapter sequences. We can remove them using [cutadapt software](https://cutadapt.readthedocs.io/en/stable/guide.html) ([docker image](https://hub.docker.com/r/kfdrc/cutadapt)).


```{bash, eval=FALSE}
for i in *.fastq; do
    echo $i
    filename="${i%.*}"
    echo $filename
    singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/kfdrc_cutadapt_latest.sif cutadapt -a AGATCGGAAGAG -o /opt/${filename}_trim.fastq /opt/$i --minimum-length 64
done
```

Check if it worked running again fastqc + multiqc

```{bash, eval=FALSE}
for i in *_trim.fastq; do # Will run in all files in the directory finishing with sub.fastq
    echo $i
    singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/biocontainers_fastqc_v0.11.9_cv7.sif fastqc /opt/$i
done

singularity run $SCRATCH/.singularity/ewels_multiqc.sif *_trim_fastqc* --title "Tutorial trim samples"
```

# Alignment

There are many software to perform the alignment. The suggested by GATK team is the BWA-MEM.

BWA requires some index files before the alignment, you can build them with:

```{bash, eval=FALSE}
docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev bwa index /opt/RchinensisV1.0/Chr04.fasta.gz

docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev  java -jar /picard.jar CreateSequenceDictionary \
    R=/opt/RchinensisV1.0/Chr04.fasta.gz \
    O=/opt/RchinensisV1.0/Chr04.dict
```

```{bash, eval=FALSE}
singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif bwa index /opt/RchinensisV1.0/Chr04.fasta.gz

singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif  java -jar /picard.jar  CreateSequenceDictionary \
    R=/opt/Chr04.fasta.gz \
    O=/opt/Chr04.dict
```
  

Run BWA-MEM for each sample file:

```{bash, eval=FALSE}
for i in *.fastq.gz; do
  echo $i
  filename="${i%%.*}"
  echo $filename
  #Alignment
  docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev bwa mem -t 2 \
    -R "@RG\tID:$filename\tLB:lib1\tPL:illumina\tSM:$filename\tPU:FLOWCELL1.LANE1" \
    /opt/RchinensisV1.0/Chr04.fasta.gz /opt/$i > $filename.bam

  #Sort BAM File
  docker run -v $(pwd):/opt kfdrc/bwa-picard:latest-dev java -jar /picard.jar SortSam \
    I="/opt/$filename.bam" \
    O="/opt/$filename.sorted.bam" \
    TMP_DIR=./tmp \
    SORT_ORDER=coordinate \
    CREATE_INDEX=true
done
```


```{bash, eval=FALSE}
for i in *_trim.fastq; do
  echo $i
  filename_temp="${i%.*}"
  filename="${filename_temp%_*}"
  echo $filename
  #Alignment
  singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif bwa mem -t 2 \
    -R "@RG\tID:$filename\tLB:lib1\tPL:illumina\tSM:$filename\tPU:FLOWCELL1.LANE1" \
    /opt/RchinensisV1.0/Chr04.fasta.gz /opt/$i > $filename.bam
    
  #Sort BAM File
  singularity run --bind $(pwd):/opt $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif java -jar /picard.jar SortSam \
    I="/opt/$filename.bam" \
    O="/opt/$filename.sorted.bam" \
    TMP_DIR=./tmp \
    SORT_ORDER=coordinate \
    CREATE_INDEX=true
done
```


If you have more than one genome and want to test how much your sample aligns with each one, [FASTQ screen](https://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/) is a good tool for that.

# GATK - Joint Call

GATK is a toolkit with more than 200 available tools. You can imagine a practical guide is not a place to explain all its features. Anyway, the Broad Institute team made really good work with tutorials on their [website](https://gatk.broadinstitute.org/hc/en-us). GATK also has workflows available [here](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows) for the most common scenarios. The bad news is that these scenarios did not evolve RADseq or GBS data sets. GATK workflows need adaptations to analyze these data sets. A good example is the duplicates, you can't remove GBS duplicates and for WGS or exome, this is an important step. If you have lots of previous information about your species, as a good reference genome or a marker database, this information can be used to increase SNP and genotype calling in GATK with tools such as BQSR and Variant recalibrator. If you don't have much previous information, GATK provides several quality measures for each marker called and you can apply what they call "hard filters" in your markers.

The called Joint Call makes the variant calling by sample and produces the g.vcf files, a VCF which have records for every position in the genome. After a database is created to join this information and we can extract it to the traditional VCF file. The analysis is done separately for several reasons, one is flexibility of processing, we can parallelize the way we want. Another is the called N+1 problem, which means that with the database created, you don't need to repeat all the analysis if you want to add an extra sample. 

```{bash, eval=FALSE}
# It requires other indexes
docker run -v $(pwd):/opt/ cristaniguti/r-samtools:latest samtools faidx /opt/RchinensisV1.0/Chr04.fasta.gz

for i in *sorted.bam; do
  filename="${i%%.*}"
  echo $filename
  docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk HaplotypeCaller \
                                                    -ERC GVCF \
                                                    -R /opt/RchinensisV1.0/Chr04.fasta.gz \
                                                    -ploidy 4 \
                                                    -I /opt/$i \
                                                    -O /opt/$filename.g.vcf \
                                                    --max-reads-per-alignment-start 0
done

zcat RchinensisV1.0/Chr04.fasta.gz | grep ">"  > interval_list_temp # Find scaffold name
sed 's/^.//' interval_list_temp > interval_list_temp2 # remove > at the beginning
awk  '{print $1;}' interval_list_temp2 > interval.list # gets only the first word

docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk GenomicsDBImport \
                                                  --genomicsdb-workspace-path /opt/my_database \
                                                  -L /opt/interval.list \
                                                  -V /opt/1.g.vcf \
                                                  -V /opt/98.g.vcf \
                                                  -V /opt/P1.g.vcf \
                                                  -V /opt/P2.g.vcf 

docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk GenotypeGVCFs \
                                                   -R /opt/RchinensisV1.0/Chr04.fasta.gz\
                                                   -O /opt/gatk.vcf.gz \
                                                   -G StandardAnnotation \
                                                   -V gendb:///opt/my_database
                                                   
```


```{bash, eval=FALSE}
# It requires other indexes
singularity exec --bind $(pwd):/opt/ $SCRATCH/.singularity/cristaniguti_r-samtools_latest.sif samtools faidx /opt/RchinensisV1.0/Chr04.fasta.gz

for i in *sorted.bam; do
  filename="${i%%.*}"
  echo $filename
  singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk HaplotypeCaller \
                                                    -ERC GVCF \
                                                    -R /opt/RchinensisV1.0/Chr04.fasta.gz\
                                                    -ploidy 4 \
                                                    -I /opt/$i \
                                                    -O /opt/$filename.g.vcf \
                                                    --max-reads-per-alignment-start 0
done

grep ">" RchinensisV1.0/Chr04.fasta.gz > interval_list_temp # Find scaffold name
sed 's/^.//' interval_list_temp > interval_list_temp2 # remove > at the beginning
awk  '{print $1;}' interval_list_temp2 > interval.list # gets only the first word

singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk GenomicsDBImport \
                                                  --genomicsdb-workspace-path /opt/my_database \
                                                  -L /opt/interval.list \
                                                  -V /opt/1.g.vcf \
                                                  -V /opt/98.g.vcf \
                                                  -V /opt/P1.g.vcf \
                                                  -V /opt/P2.g.vcf 

singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk GenotypeGVCFs \
                                                   -R /opt/RchinensisV1.0/Chr04.fasta.gz \
                                                   -O /opt/gatk.vcf.gz \
                                                   -G StandardAnnotation \
                                                   -V gendb:///opt/my_database
                                                   
```

# WDL workflow

To automatize all these interactions with HPC and cloud, we can use a workflow system. There are several available. The Broad Institute team develop a combination of the WDL workflow language with workflow manager Cromwell or Terra.bio web platform, which already solved a lot of technical issues evolved with the analysis of biological sequences in Cloud or HPC. See [Genomics in the Cloud book](https://www.amazon.com.br/Genomics-Cloud-Brian-%E2%80%B2connor/dp/1491975199) and GATK tutorials.

* [WDL for this tutorial](https://drive.google.com/drive/folders/1Wc6LgL_hqi-gx7JVl1IlEOxvJ93CuTgP?usp=sharing)
* [Configuration for SLURM and Singularity](https://drive.google.com/drive/folders/1Wc6LgL_hqi-gx7JVl1IlEOxvJ93CuTgP?usp=sharing)
* [samples_info](https://drive.google.com/drive/folders/1Wc6LgL_hqi-gx7JVl1IlEOxvJ93CuTgP?usp=sharing)
* [Download cromwell](https://github.com/broadinstitute/cromwell/releases)
* [Download womtool](https://github.com/broadinstitute/cromwell/releases)

Get genome index files:

Prepare input file:

```{bash, eval=FALSE}
java -jar womtool.jar run /scratch/user/chtaniguti/tetra_examples/gatk_polyploid.wdl > gatk_polyploid.inputs.json
```

```


```

```{bash, eval=FALSE}
java -jar -Dconfig.file=/scratch/user/chtaniguti/cromwell_sing_slurm.conf -jar /scratch/user/chtaniguti/cromwell.jar run /scratch/user/chtaniguti/tetra_examples/gatk_polyploid.wdl -i /scratch/user/chtaniguti/tetra_examples/gatk_polyploid.inputs.json 
```

* [Terra.bio](https://terra.bio/) is connected with two repositories of WDL workflows that contains all type of analysis.


## Dosage calling

Install the R packages:

```{r, eval=FALSE}
install.packages("polyRAD")
install.packages("updog")
# install.packages("fitPoly") # Array data


install.packages("vcfR") # To manipulate VCF file inside R
```

Check their documentation:

* [polyRAD](https://github.com/lvclark/polyRAD)
* [updog](https://dcgerard.github.io/updog/)
* [fitPoly](https://www.wur.nl/en/show/Software-fitPoly.htm)

Example considering a $F_1$ outcrossing population:

### updog

```{r}
library(vcfR)

vcf <- read.vcfR("vcf_poly_filt.vcf.gz")
vcf

sizemat <- extract.gt(vcf, "DP")

ADmat <- extract.gt(vcf, "AD")
refmat <- strsplit(ADmat, ",")
refmat <- sapply(refmat, "[[", 1)
refmat <- matrix(refmat, nrow = nrow(ADmat))
refmat[1:5, 1:5]
ADmat[1:5,1:5]

library(updog)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

sizemat <- apply(sizemat, 2, as.numeric)
refmat <- apply(refmat, 2, as.numeric)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

colnames(refmat) <- colnames(sizemat) <- colnames(ADmat)
rownames(refmat) <- rownames(sizemat) <- rownames(ADmat)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

plot(mout, indices = c(1, 1, 10))

genomat <- format_multidog(mout, varname = "geno")
head(genomat)
```


### polyRAD

```{r}
library(polyRAD)

mydata <- VCF2RADdata(myVCF, possiblePloidies = list(2, c(2,2)),
                      expectedLoci = 100, expectedAlleles = 500,
                      taxaPloidy = pld_vect)


# Quality control and parameter estimation
overdispersionP <- TestOverdispersion(mydata, to_test = 8:14)


```




